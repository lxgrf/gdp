{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217f9cf7-4a1d-4a0b-9499-8350ba781676",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1b2495-e8d5-4f72-bc5e-85c1b4972cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import trt_pose.coco\n",
    "\n",
    "with open('human_pose.json', 'r') as f:\n",
    "    human_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)\n",
    "\n",
    "keypoints = [\"nose\", \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\", \"left_shoulder\",\n",
    "    \"right_shoulder\", \"left_elbow\", \"right_elbow\", \"left_wrist\", \"right_wrist\",\n",
    "    \"left_hip\", \"right_hip\", \"left_knee\", \"right_knee\", \"left_ankle\", \"right_ankle\", \"neck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d39f511-9fe6-4134-8ccc-c96aae0fbcf5",
   "metadata": {},
   "source": [
    "### Pose Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8035c0-f769-4b19-86dc-9a46af05a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"best.pkl\",'rb')\n",
    "posemodel = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb209a-7628-442e-9b0f-7653d76daca3",
   "metadata": {},
   "source": [
    "### Box Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6adeb7e9-ca8c-4d48-975c-49df88fffcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mscai/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2021-8-10 torch 1.8.0 CUDA:0 (NVIDIA Tegra X1, 3964.1328125MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7056607 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "boxmodel = torch.hub.load('ultralytics/yolov5', 'custom', path='box.pt')\n",
    "boxmodel.conf = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7fe7ca-219a-4496-8834-0a244d81d8c5",
   "metadata": {},
   "source": [
    "### Pose Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a76cb6-6b98-4fec-aaf8-47d725bf3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trt_pose.models\n",
    "\n",
    "num_parts = len(human_pose['keypoints'])\n",
    "num_links = len(human_pose['skeleton'])\n",
    "\n",
    "model = trt_pose.models.resnet18_baseline_att(num_parts, 2 * num_links).cuda().eval()\n",
    "\n",
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "\n",
    "data = torch.zeros((1, 3, HEIGHT, WIDTH)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca70be5-5e91-4741-ba03-16f0d3868b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch2trt import TRTModule\n",
    "OPTIMIZED_MODEL = 'resnet18_baseline_att_224x224_A_epoch_249_trt.pth'\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "763d6972-8bef-4222-b89e-4fd095ead7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d06c5f5e-4517-4505-a522-2515b9fed0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c25c1-443b-4dba-bf62-9486066da880",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50cd4ea4-b073-4cae-a975-b8de89da7ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    global device\n",
    "    device = torch.device('cuda')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]\n",
    "\n",
    "def scalepose(pose):\n",
    "    means = np.array([np.mean(pose[:,0]),np.mean(pose[:,1])])\n",
    "    stds = np.array([np.std(pose[:,0]),np.std(pose[:,1])])\n",
    "    if np.count_nonzero(stds) != 2: return pose # Bail out on an empty Skeleton\n",
    "    pose = (pose.T - means[:,None]) / stds[:,None]\n",
    "    return pose.T\n",
    "\n",
    "def pose_class(peaks, counts, threshold, parts, bounds):  # TODO extract left and right wrist\n",
    "    npeaks = peaks.numpy()[0]\n",
    "    skeletons, skpresence, boxtype = list(), list(), list()\n",
    "    if counts[0] == 0: y_pred = [-1]\n",
    "    for i in range(counts[0]):\n",
    "        if np.count_nonzero(npeaks[:,i]) >= threshold:\n",
    "            skeleton = scalepose(npeaks[:,i]).flatten()\n",
    "            skeletons.append(skeleton)\n",
    "            skpresence.append(True)\n",
    "            boxtype.append(box_interact(parts, bounds, i))\n",
    "        else: skpresence.append(False)\n",
    "    if len(skeletons)>0: \n",
    "        pred = posemodel.predict(skeletons)\n",
    "        y_pred = list()\n",
    "        for p, b in zip(pred,boxtype):\n",
    "            if p == 1:\n",
    "                if b == 'heavy_box': y_pred.append(2)\n",
    "                else: y_pred.append(1) # Includes regular box and no box\n",
    "            elif p == 2: y_pred.append(3)\n",
    "            else: y_pred.append(p)\n",
    "    else: \n",
    "        y_pred = [-1]\n",
    "    return skpresence, y_pred, skeletons\n",
    "\n",
    "# 0 = Correct\n",
    "# 1 = Incorrect, light box\n",
    "# 2 = Incorrect, heavy box\n",
    "# 3 = Nonlifting\n",
    "\n",
    "def box_interact(parts, bounds, i):\n",
    "    boxtype = None\n",
    "    lw = (parts[i]['left_wrist']['x'], parts[i]['left_wrist']['y'])\n",
    "    rw = (parts[i]['right_wrist']['x'], parts[i]['right_wrist']['y'])\n",
    "    for key in bounds:\n",
    "        box = bounds[key]\n",
    "        try:\n",
    "            if None not in lw:\n",
    "                if lw[0] > box['x1'] and lw[0] < box['x2'] and lw[1] > box['y1'] and lw[1] < box['y2'] and boxtype == None:\n",
    "                    boxtype = box['name']\n",
    "            if None not in rw:\n",
    "                if rw[0] > box['x1'] and rw[0] < box['x2'] and rw[1] > box['y1'] and rw[1] < box['y2'] and boxtype == None:\n",
    "                    boxtype = box['name']\n",
    "        except: boxtype = None\n",
    "    return boxtype\n",
    "\n",
    "def kpoints(objects, counts, peaks):\n",
    "    kpoint_total = dict()\n",
    "    for i in range(counts[0]):\n",
    "        kpoint_local = dict()\n",
    "        human = objects[0][i]\n",
    "        C = human.shape[0]\n",
    "        for j in range(C):\n",
    "            k = int(human[j])\n",
    "            if k >= 0:\n",
    "                peak = peaks[0][j][k]\n",
    "                y = float(peak[0])\n",
    "                x = float(peak[1])\n",
    "                kpoint_local.update({keypoints[j]:{'x':x, \n",
    "                                                   'y':y}})\n",
    "            else: kpoint_local.update({keypoints[j]:{'x':None,\n",
    "                                                     'y':None}})\n",
    "        kpoint_total.update({i:kpoint_local})\n",
    "    return kpoint_total\n",
    "\n",
    "def execute(change):\n",
    "    # Outputs same size as input, resizes copy for inference.\n",
    "    image = cv2.flip(change['new'], 0)\n",
    "\n",
    "    # Find boxes\n",
    "    boxResults = boxmodel([np.flip(image,2)])\n",
    "    bounds = bounding_boxes(boxResults)\n",
    "    \n",
    "    # Find Poses\n",
    "    data = preprocess(cv2.resize(image,(HEIGHT,WIDTH)))\n",
    "    cmap, paf = model_trt(data)\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf)\n",
    "    parts = kpoints(objects, counts, peaks)\n",
    "    \n",
    "    # Classify Poses with boxes\n",
    "    skpresence, y_pred, skeletons = pose_class(peaks, counts, 6, parts, bounds)\n",
    "    #statusupdate(status, skpresence, y_pred, skeletons)\n",
    "    \n",
    "    # Annotate Image\n",
    "    draw_objects(image, counts, objects, peaks, skpresence, y_pred)\n",
    "    image = bounding_box_draw(image, bounds)\n",
    "    \n",
    "    if now:\n",
    "        timetaken = datetime.now() - now\n",
    "        fps = 1/timetaken\n",
    "        with status: print('FPS = {fps}')\n",
    "        now = datetime.now()\n",
    "    \n",
    "    # Output\n",
    "    image_w.value = bgr8_to_jpeg(image[:, ::-1, :])\n",
    "\n",
    "    \n",
    "def execute_vid(image):\n",
    "    # Outputs same size as input, resizes copy for inference.\n",
    "    \n",
    "    image = cv2.flip(image,1)\n",
    "    \n",
    "    # Find boxes\n",
    "    boxResults = boxmodel([np.flip(image,2)])\n",
    "    bounds = bounding_boxes(boxResults)\n",
    "    \n",
    "    # Find Poses\n",
    "    data = preprocess(cv2.resize(image,(HEIGHT,WIDTH)))\n",
    "    cmap, paf = model_trt(data)\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf)#, cmap_threshold=0.15, link_threshold=0.15)\n",
    "    parts = kpoints(objects, counts, peaks)\n",
    "    \n",
    "    skpresence, y_pred, skeletons = pose_class(peaks, counts, 8, parts, bounds)\n",
    "    #statusupdate(status, skpresence, y_pred, skeletons)\n",
    "    \n",
    "    # Annotate Image\n",
    "    image = bounding_box_draw(image, bounds)\n",
    "    draw_objects(image, counts, objects, peaks, skpresence, y_pred)\n",
    "    \n",
    "    if now:\n",
    "        timetaken = datetime.now() - now\n",
    "        fps = 1/timetaken\n",
    "        with status: print('FPS = {fps}')\n",
    "        now = datetime.now()\n",
    "    \n",
    "    # Return Image\n",
    "    return bgr8_to_jpeg(image[:, ::-1, :])\n",
    "    \n",
    "def statusupdate(status, skpresence, y_pred, skeletons):\n",
    "    pred = [\"Correct\", \"Incorrect\", \"Incorrect - Heavy Box\" \"Nonlifting\"]\n",
    "    status.clear_output()\n",
    "    with status: \n",
    "        print(f\"Poses detected = {len(skpresence)}\\nPoses Classified = {len(y_pred)}\")\n",
    "    if len(skpresence)>=1:\n",
    "        for i in range(len(skpresence)):\n",
    "            with status: \n",
    "                print(pred[y_pred[i]] + \"\\n\")\n",
    "                print(skeletons[i])\n",
    "    else:\n",
    "        with status: print(\"No poses detected.\")\n",
    "\n",
    "def bounding_boxes(detections):\n",
    "    names = detections.names\n",
    "    detections = detections.xyxyn[0].tolist()\n",
    "    boxes = dict()\n",
    "    for i in range(len(detections)):\n",
    "        d = detections[i]\n",
    "        boxes.update({i:{\n",
    "            'name':names[int(d[5])],\n",
    "            'x1':d[0],\n",
    "            'y1':d[1],\n",
    "            'x2':d[2],\n",
    "            'y2':d[3],\n",
    "            }})\n",
    "    return boxes\n",
    "\n",
    "def bounding_box_draw(image, bounds):\n",
    "    for key in bounds.keys():\n",
    "        h, w, _ = image.shape\n",
    "        if bounds[key]['name'] == 'heavy_box': color = (0, 0, 255)\n",
    "        else: color = (0, 255, 0)\n",
    "        x1 = int(bounds[key]['x1']*w)\n",
    "        x2 = int(bounds[key]['x2']*w)\n",
    "        y1 = int(bounds[key]['y1']*h)\n",
    "        y2 = int(bounds[key]['y2']*h)\n",
    "        image = cv2.rectangle(image, (x1,y1), (x2,y2), color = color, thickness = 5)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d45a63-57eb-4b92-9f40-e9b42bb6173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdp_drawobjects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "parse_objects = ParseObjects(topology)\n",
    "draw_objects = DrawObjects(topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1a41e-5a8e-462e-a667-d254463bb670",
   "metadata": {},
   "source": [
    "## Establish Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d838f74-5b79-4ca5-9a71-246431d64644",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not initialize camera.  Please see error trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/gdp/lib/python3.6/site-packages/jetcam-0.0.0-py3.6.egg/jetcam/csi_camera.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Could not read image from camera.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not read image from camera.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-16efbcaa042b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#camera = CSICamera(width=WIDTH, height=HEIGHT, capture_fps=30)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcamera\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSICamera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1280\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m720\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_fps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gdp/lib/python3.6/site-packages/jetcam-0.0.0-py3.6.egg/jetcam/csi_camera.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             raise RuntimeError(\n\u001b[0;32m---> 27\u001b[0;31m                 'Could not initialize camera.  Please see error trace.')\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not initialize camera.  Please see error trace."
     ]
    }
   ],
   "source": [
    "from jetcam.csi_camera import CSICamera\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "\n",
    "#camera = CSICamera(width=WIDTH, height=HEIGHT, capture_fps=30)\n",
    "camera = CSICamera(width=1280, height=720, capture_fps=30)\n",
    "\n",
    "camera.running = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148f9c1-e9dc-490a-8e65-af2bbdffc31c",
   "metadata": {},
   "source": [
    "## Output Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9f6757f-273c-44d7-8e8a-fb96c190b11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f23b927356413284e768e353930bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1940f216c77241a0a6bc3428adc5b82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "\n",
    "image_w = ipywidgets.Image(format='jpeg')\n",
    "status = ipywidgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "display(image_w)\n",
    "display(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d41216-0db9-4ae3-9dca-7e4523684399",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "206c45d7-f29e-47cb-84f2-6335778c7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Frame\n",
    "\n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fdbd83c-10ee-4a98-9a69-eeff18dff807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous\n",
    "\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cf84aec-67af-45c1-bc94-a05f09114732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Continuous\n",
    "\n",
    "camera.unobserve_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0ef16ee-2c1f-4863-9cde-96262fd36340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release Camera (Do this when you're done or you might struggle to start up again)\n",
    "\n",
    "camera.running = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba6be30-cf70-4139-b3d2-4f05be4cf2ca",
   "metadata": {},
   "source": [
    "## Below this is pre-recorded Video Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a72b6bc5-37e2-480b-9e23-f837ea743b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TS008_01.mp4',\n",
       " 'TS004_01.mp4',\n",
       " 'TS005_01.mp4',\n",
       " 'inferred.avi',\n",
       " 'TS003_02.mp4',\n",
       " 'TS005_02.mp4',\n",
       " 'TS007_02.mp4',\n",
       " 'TS001_01.mp4',\n",
       " 'TS006_01.mp4',\n",
       " 'TS002_01.mp4',\n",
       " 'TS001_02.mp4',\n",
       " 'TS007_01.mp4',\n",
       " 'TS003_01.mp4',\n",
       " 'video.ipynb']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "import matplotlib.pyplot as plt\n",
    "vidpath = '../../../video'\n",
    "vids = os.listdir('../../../video')\n",
    "vids = [vid for vid in vids if vid[0] is not '.']\n",
    "vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35ea89a9-d797-4337-9494-1b615461ed12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'name': 'box', 'x1': 0.44218748807907104, 'y1': 0.40414267778396606, 'x2': 0.91796875, 'y2': 0.8484786152839661}}\n"
     ]
    }
   ],
   "source": [
    "image_w.value = execute_vid(cv2.imread('../../../training_images/incorrect/0010.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23f8d302-d7e4-41f7-9e10-ac343e331b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "vidfile = 'TS008_01.mp4'\n",
    "\n",
    "vid = cv2.VideoCapture(os.path.join(vidpath,vidfile))\n",
    "#out = cv2.VideoWriter(os.path.join(vidpath,'inferred.avi'),cv2.VideoWriter_fourcc(*'DIVX'), 25, (1280,720))\n",
    "\n",
    "while True:\n",
    "    ret_val, frame = vid.read()\n",
    "    if ret_val:\n",
    "        output = execute_vid(frame)\n",
    "        image_w.value = output\n",
    "        #out.write(output)\n",
    "    else:\n",
    "        break\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break  # esc to quit\n",
    "        \n",
    "#out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
